# -*- coding: utf-8 -*-
"""tree1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fdkig6W5VMjv__NuPUkNk8vK4eo10L5F
"""

from __future__ import print_function 
import numpy as np 
import pandas as pd 
import random
import sys
from collections import Counter

class TreeNode(object):
  def __init__(self, ids = None, children = [], entropy = 0, depth = 0):
    self.ids = ids           # this node에 해당되는 data의 행의 인덱스값들
    self.entropy = entropy   # entropy 
    self.depth = depth       # root node 까지의 거리
    self.split_attribute = None # attribute for spliting 
    self.children = children # child nodes's list
    self.split_values = None       # list of values of split_attribute in children
    self.label = None       # label of node if it is a leaf
  
  def set_properties(self, split_attribute, split_values):
    self.split_attribute = split_attribute
    self.split_values = split_values

  # leaf node인경우, 해당 node에 label를 설정
  def set_label(self, label):
    self.label = label

def entropy(freq) :
    # 개수가 0인 항목은 배제한다.
    freq_idx = np.array(freq.nonzero()[0])
    freq = freq[freq_idx]

    # set each element's probabilty
    prob= freq/float(freq.sum())

    # entropy
    return -np.sum(prob*np.log2(prob))

class DecisionTree(object):
  def __init__ (self, max_depth= 10, min_samples_split = 2, min_gain = 1e-4) :
    self.root = None # root node
    self.max_depth = max_depth  # maximum depth
    self.min_samples_split = min_samples_split # minimum number of data to be spli
    self.Ndata = 0 # number of data
    self.min_gain = min_gain #minimum gain

  # pandas data의 인덱스를 1부터 len(data)까지 설정하기
  def idx_reset(self, pd_data) : 
    pd_data.reset_index(drop=True, inplace=True)
    pd_data.index=pd_data.index + 1
    return pd_data

  def fit(self, data, target):
    self.Ndata = data.count()[0] 
    self.data = self.idx_reset(data) 
    self.target = self.idx_reset(target) 
    self.attributes = list(data) # train data로부터 속성들 추출
    self.labels = target.unique() # target data로부터 속성 values 추출
    ids = range(self.Ndata)  # array of indexes
    self.root = TreeNode(ids = ids, entropy = self.node_entropy(ids), depth = 0)
  
    queue = [self.root]
    while queue : # recursive partitioning
      node = queue.pop()
      if node.depth < self.max_depth or node.entropy < self.min_gain : 
        node.children = self.split_node(node)
        if not node.children : 
          self._set_label(node) # leaf node에 label 설정
        queue += node.children  # node 탐색
      else :
        self._set_label(node) # leaf node에 label 설정

  def node_entropy(self, ids) :
     # calculate entropy of a node with index ids
     if len(ids) == 0: return 0
     ids = [i+1 for i in ids] # pandas series index starts from 1
     freq = np.array(self.target[ids].value_counts()) #해당 속성에서, target 값의 비율 계산하기 
     return entropy(freq)
  
  def _set_label(self, node) :
    # find label for a node if it is a leaf
    # target is a series variable @
   target_ids = [i+1 for i in node.ids] 
    
    #  major voting를 통해 제일 많이 나온 값을 node의 최종값으로 설정
   node.set_label(self.target[target_ids].mode()[0]) # 


  def split_node(self, node) :
    ids = node.ids
    best_gain = 0 # best inforamation gain를 저장할 변수
    best_splits = []
    best_attribute = None
    split_values = None
    sub_data = self.data.iloc[ids, :]

    #속성들 중에서, informaion gain을 구해서 분기를 한다.
    for i, att in enumerate(self.attributes):
      values = self.data.iloc[ids, i].unique().tolist() # ids에 속하는 해당 속성의 값들의 종류
      if len(values) == 1: # entropy = 0
        continue 
      splits = []

      for val in values : # 속성에서 각각의 value에 해당하는 ids를 split에 추가
        sub_ids = sub_data.index[sub_data[att] == val].tolist()
        splits.append([sub_id-1 for sub_id in sub_ids])

      # split된 항목들의 개수가 min_samples_split보다 작으면 그냥 넘기기
      if min(map(len, splits)) < self.min_samples_split: 
        continue
    
      #information gain 구하기
      weighted_entropy = 0
      for split in splits:
        total_num = len(ids) #부모노드에 속한 총 개수
        split_num = len(split) # split 노드 개수
        weighted_entropy += split_num*self.node_entropy(split)/total_num
      info_gain = node.entropy -  weighted_entropy
      
      #information gain이 minimum gain보다 낮으면, 해당 속성의 분기는 고려하지 않음 
      if info_gain < self.min_gain : continue 
      
      #information gain 비교
      if info_gain > best_gain :
        best_gain = info_gain
        best_splits = splits 
        best_attribute = att
        split_values = values

    # 노드에 best attribute 속성과, 해당 값들을 property로 설정
    node.set_properties(best_attribute, split_values)
    
    # 나눠진 분기점들을 다시 node로 설정
    child_nodes = [TreeNode(ids=split, entropy = self.node_entropy(split), depth = node.depth+1) for split in best_splits]
    return child_nodes

  def classify(self, test_data) :
    # test_data가 series 타입일 경우, dataframe으로 바꿔준다.
    if len(test_data.shape) == 1:
      test_data =test_data.to_frame().transpose()
   
    data_num = len(test_data) # test_data 개수    
    labels = [] # prediction를 저장할 변수  
    for n in range(data_num):
        x = test_data.iloc[n,:] # 1개의 데이터
        node = self.root
        while node.children :
          attribute = node.split_attribute
          # 해당 데이터 속성의 value가 학습되지 않은 경우, 학습된 values중에서 임의로 추출한다.
          if x[attribute] not in node.split_values : 
            x[attribute] = random.sample(node.split_values,1)[0]

          # 해당 속성에 있는 value값을 기준으로 분기를 한다.      
          value_idx = node.split_values.index(x[attribute]) 
          node = node.children[value_idx]      
        labels.append(node.label)

    return labels